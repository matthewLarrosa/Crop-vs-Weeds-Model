{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, InputLayer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_crop_weeds = 'agri_data/data/'  # Adjust this path to your dataset\n",
    "\n",
    "# Image Sizes for MobileNetV2\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Defines batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (img_width, img_height))\n",
    "    return image / 255.0\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        class_label = int(file.readline().strip().split()[0])\n",
    "    return class_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "annotations = []\n",
    "\n",
    "for filename in os.listdir(dataset_crop_weeds):\n",
    "    if filename.endswith('.jpeg'):\n",
    "        image_path = os.path.join(dataset_crop_weeds, filename)\n",
    "        images.append(image_path)\n",
    "    elif filename.endswith('.txt'):\n",
    "        annotation_path = os.path.join(dataset_crop_weeds, filename)\n",
    "        annotations.append(annotation_path)\n",
    "\n",
    "# Split the data\n",
    "train_images, test_images, train_annotations, test_annotations = train_test_split(images, annotations, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_annotations, val_annotations = train_test_split(train_images, train_annotations, test_size=0.25, random_state=42)\n",
    "\n",
    "train_images_data = np.array([load_and_preprocess_image(img) for img in train_images])\n",
    "val_images_data = np.array([load_and_preprocess_image(img) for img in val_images])\n",
    "test_images_data = np.array([load_and_preprocess_image(img) for img in test_images])\n",
    "\n",
    "train_annotations_data = np.array([parse_annotation(ann) for ann in train_annotations])\n",
    "val_annotations_data = np.array([parse_annotation(ann) for ann in val_annotations])\n",
    "test_annotations_data = np.array([parse_annotation(ann) for ann in test_annotations])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "train_datagen = ImageDataGenerator(**data_gen_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 9s 319ms/step - loss: 0.8477 - accuracy: 0.4923 - val_loss: 0.7133 - val_accuracy: 0.4731\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 7s 274ms/step - loss: 0.7675 - accuracy: 0.5321 - val_loss: 0.7134 - val_accuracy: 0.5038\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 0.7726 - accuracy: 0.5167 - val_loss: 0.7227 - val_accuracy: 0.5038\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 6s 247ms/step - loss: 0.7344 - accuracy: 0.5256 - val_loss: 0.7264 - val_accuracy: 0.4808\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 6s 243ms/step - loss: 0.7207 - accuracy: 0.5436 - val_loss: 0.7528 - val_accuracy: 0.4962\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 6s 245ms/step - loss: 0.7695 - accuracy: 0.4936 - val_loss: 0.7374 - val_accuracy: 0.4962\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 6s 246ms/step - loss: 0.7312 - accuracy: 0.5538 - val_loss: 0.7578 - val_accuracy: 0.5154\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 6s 248ms/step - loss: 0.7181 - accuracy: 0.5423 - val_loss: 0.7325 - val_accuracy: 0.5231\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 6s 245ms/step - loss: 0.7018 - accuracy: 0.5949 - val_loss: 0.7382 - val_accuracy: 0.4500\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 6s 245ms/step - loss: 0.6979 - accuracy: 0.5679 - val_loss: 0.7586 - val_accuracy: 0.4462\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_datagen.flow(train_images_data, train_annotations_data, batch_size=batch_size), epochs=epochs, validation_data=(val_images_data, val_annotations_data), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 1s - loss: 0.7463 - accuracy: 0.4962 - 1s/epoch - 167ms/step\n",
      "Test accuracy: 0.4961538314819336\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images_data, test_annotations_data, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
